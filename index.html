<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="keywords"
      content="egocentric, body motion, body tracking, motion synthesis, action recognition"
    />
    <title>EgoMotion Workshop</title>
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@400&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="css/styles.css" />
  </head>

  <body>
    <div class="header">
      <picture>
        <source srcset="css/teaser.jpg" type="image/jpeg" />
        <img src="css/teaser.jpg" alt="teaser cover" class="responsive-image" />
      </picture>
      <p>the 2nd Workshop on</p>
      <h1 style="color: #f4976c; font-size: 28px">EgoMotion</h1>
      <h1 style="color: #d2fdff; font-size: 38px">
        Egocentric Body Motion Tracking, Synthesis and Action Recognition
      </h1>
      <p>
        <i class="fa-solid fa-calendar-day fa-beat" style="color: #f4976c"></i>
        &nbsp;October 20th 13:00 - 17:00 PST
      </p>
      <p>
        <i class="fa-solid fa-location-dot fa-beat" style="color: #f4976c"></i
        >&nbsp;ICCV 2025, Ballroom C, Hawaii Convention Center, Level 4
      </p>
    </div>

    <main>
      <section id="overview" class="dark-block">
        <div class="overview-container">
          <h2>Context</h2>
          <p>
            <!-- The EgoMotion workshop focuses on in-the-wild egocentric full-body
            motion understanding from data captured by wearable devices. We
            address three major problems for full-body motion -- tracking,
            synthesis and action recognition. The scope combines several topics
            and sub-fields that have grown in importance over the last years,
            and aims to bring these fields together in a common forum. Towards
            egocentric full-body tracking, motion synthesis and action
            recognition, we will focus on, but not limited to, algorithms
            developed for the following inputs. -->
            EgoMotion is the 2nd edition of EgoMotion Workshop for in-the-wild human motion modeling using
            egocentric multi-modal data from wearable devices. We focus on topics and sub-fields in motion
            tracking, synthesis, and understanding. These research topics have garnered increasing interest and
            momentum thanks to recent advances in wearable egocentric sensors. Specific topics include:
          </p>
          <ul>
            <li>
              Body tracking, synthesis, and activity understanding from egocentric/exocentric cameras
            </li>
            <li>
              Body tracking, synthesis, and activity understanding from non-visual wearable sensors, e.g., inertial measurement units (IMUs),
              electromagnetic (EM) sensors, ultra-wideband (UWB) sensors, radar, and audio.
            </li>
            <li>
              Applications of egocentric motion for character animation, simulation, robotic learning, etc.
            </li>
          </ul>
          <p>
            In addition to algorithms, the workshop aims to promote recent open-source projects to encourage
            and accelerate research in this field. To this end, we invite researchers to present recent large-scale mo-
            tion datasets and associated challenges, such as GORP, Nymeria, EgoLife, HP-EPIC,
            EgoBody, etc. The workshop will also discuss state-of-the-art human motion modeling libraries
            such as Meta Momentum, present research hardware platforms such as Project Aria, and
            organize live demos.
          </p>
        </div>
      </section>

      <section id="speaker" class="light-block">
        <div class="promo-container">
          <video autoplay loop muted playsinline class="responsive-image" poster="people/egomotion-promo.webp">
            <source src="people/egomotion_promo_2025.mp4" type="video/mp4" />
            Your browser does not support the video tag.
          </video>
        </div>
        <div class="speaker-container">
          <h2>Invited Speakers</h2>
        </div>
        <div class="speaker-container">
          <div>
            <img src="people/ziwei.png" alt="Ziwei Liu" />
            <div>
              <a href="https://liuziwei7.github.io/"
                >Ziwei Liu</a
              >
            </div>
            <div class="inst">Assistant Professor<br />NTU</div>
          </div>
          <div>
            <img src="people/angjoo.jpg" alt="Angjoo Kanazawa" />
            <div>
              <a href="https://people.eecs.berkeley.edu/~kanazawa/"
                >Angjoo Kanazawa</a
              >
            </div>
            <div class="inst">
              Assistant Professor<br />
              UC Berkeley
            </div>
          </div>
          <div lang="de">
            <img src="people/danfei.jpg" alt="Danfei Xu" />
            <div>
              <a href="https://faculty.cc.gatech.edu/~danfei/">
                Danfei Xu</a
              >
            </div>
            <div class="inst">
              Assistant Professor<br />
              Georgia Tech
            </div>
          </div>
        </div>
        <div class="speaker-container">
          <div>
            <img src="people/ruihan.jpg" alt="Ruihan Yang" />
            <div>
              <a
                href="https://rchalyang.github.io/"
                >Ruihan Yang</a
              >
            </div>
            <div class="inst">
              Applied Scientist<br />
              Amazon Frontier AI & Robotics (FAR)
            </div>
          </div>
          <div>
            <img src="people/jian.png" alt="Jian Wang" />
            <div>
              <a href="https://jianwang-mpi.github.io/">Jian Wang</a>
            </div>
            <div class="inst">
              Research Scientist<br /> Meta
            </div>
          </div>
          <div>
            <img src="people/boxiao.jpg" alt="Boxiao Pan" />
            <div>
              <a href="https://cs.stanford.edu/~bxpan/">Boxiao Pan</a>
            </div>
            <div class="inst">Research Scientist<br />Luma AI</div>
          </div>
        </div>
        <div class="speaker-container">
          <div>
            <img src="people/gen.png" alt="Gen Li" />
            <div>
              <a href="https://ligengen.github.io/">Gen Li</a>
            </div>
            <div class="inst">
              Ph.D Student<br />
              ETH Zurich
            </div>
          </div>
          <div>
            <img src="people/ziqi.png" alt="Ziqi Huang" />
            <div>
              <a href="https://ziqihuangg.github.io/">Ziqi Huang</a>
            </div>
            <div class="inst">
              Ph.D Student<br />
              NTU
            </div>
          </div>
        </div>
      </section>

      <section id="agenda" class="dark-block">
        <div class="agenda-container">
          <h2>Schedule</h2>
        </div>
        <div class="agenda-container">
          <table>
            <colgroup>
              <col style="width: 12%" />
              <col style="width: 12%" />
              <col style="width: 55%" />
            </colgroup>
            <tr>
              <th>Time</th>
              <th>Event</th>
              <th>Speaker</th>
            </tr>
            <tr>
              <td>13:00 - 13:30</td>
              <td>Opening Talk</td>
              <td><p class="speaker-highlight">Richard Newcombe</p></td>
            </tr>
            <tr>
              <td>13:30 - 14:05</td>
              <td>Keynote</td>
              <td>
                <p class="speaker-highlight">Ziwei Liu, Ziqi Huang </p>
                <p>From Egocentric Perception to Embodied Intelligence: Building the World in First Perso</p>
                <p></p>
              </td>
            </tr>
            <tr>
              <td>14:05 - 14:40</td>
              <td>Keynote</td>
              <td>
                <p class="speaker-highlight">Angjoo Kanazawa</p>
                <p>Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop</p>
              </td>
            </tr>
            <tr>
              <td>14:40 - 15:00</td>
              <td>Break</td>
                            <!-- <td> -->
                <!-- <i class="fa-solid fa-fire fa-beat" style="color: #f4976c"></i> -->
                <!-- &nbsp;Live demos with Quest and Project Aria -->
              <!-- </td> -->
            </tr>
            <tr>
              <td>15:00 - 15:35</td>
              <td>Keynote</td>
              <td>
                <p class="speaker-highlight">Danfei Xu</p>
                <p>Human Experience as a Foundation for Robot learning</p>
              </td>
            </tr>
            <tr>
              <td>15:35 - 15:48</td>
              <td>Tech Talk</td>
              <td>
                <p class="speaker-highlight">Lingni Ma</p>
                <p>Nymeria++: Upgrade to the Nymeria Dataset with Optimized Motion, 3D Objects and More</p>
              </td>
            </tr>
            <tr>
              <td>15:48 - 16:02</td>
              <td>Tech Talk</td>
              <td>
                <p class="speaker-highlight">Ruihan Yang</p>
                <p>Title TBD</p>
              </td>
            </tr>
            <tr>
              <td>16:01 - 16:14</td>
              <td>Tech Talk</td>
              <td>
                <p class="speaker-highlight">Jian Wang</p>
                <p>
                  Egocentric Motion Capture and Understanding from Multi-Modal Inputs
                </p>
              </td>
            </tr>
            <tr>
              <td>16:14 - 16:27</td>
              <td>Tech Talk</td>
              <td>
                <p class="speaker-highlight">Boxiao Pan</p>
                <p>Real-World Humanoid Egocentric Navigation</p>
              </td>
            </tr>
            <tr>
              <td>16:27 - 16:40</td>
              <td>Tech Talk</td>
              <td>
                <p class="speaker-highlight">Gen Li</p>
                <p>Towards Egocentric Multimodal Multitask Pretraining</p>
              </td>
            </tr>
            <tr>
              <td>16:40 - 16:53</td>
              <td>Tech Talk</td>
              <td>
                <p class="speaker-highlight">Robin Kips</p>
                <p>Egocentric Motion Synthesis and GORP dataset</p>
              </td>
            </tr>
            <tr>
              <td>16:55 - 17:00</td>
              <td>Closing</td>
              <td>&nbsp;</td>
            </tr>
          </table>
        </div>
      </section>

      <section id="organizer" class="light-block">
        <h2>Organizers</h2>
        <div class="organizer-container">
          <div>
            <img src="people/lingni.jpg" alt="Lingni Ma" />
            <div>
              <a
                href="https://scholar.google.nl/citations?user=eUAgpwkAAAAJ&hl=en"
                >Lingni Ma</a
              >
            </div>
            <div class="inst">Research Scientist<br />Meta</div>
          </div>
          <div>
            <img src="people/yuting.jpg" alt="Yuting Ye" />
            <div>
              <a href="http://yutingye.info/">Yuting Ye </a>
            </div>
            <div class="inst">Director, Research Scientist<br />Meta</div>
          </div>
          <div>
            <img src="people/robin.jpg" alt="Robin Kips" />
            <div>
              <a href="https://scholar.google.nl/citations?user=RwyrWEkAAAAJ&hl=en">Robin Kips</a>
            </div>
            <div class="inst">Research Scientist<br />Meta</div>
          </div>
          <div>
            <img src="people/james_fort.png" alt="James Fort" />
            <div>
              <a href="https://www.linkedin.com/in/jamesfort4/"
                >James Fort</a
              >
            </div>
            <div class="inst">Research Product Manager<br />Meta</div>
          </div>

        </div>
        <div class="organizer-container">
          <div>
            <img src="people/siyu.jpg" alt="Siyu Tang" />
            <div>
              <a href="https://vlg.inf.ethz.ch/team/Prof-Dr-Siyu-Tang.html">Siyu Tang</a>
            </div>
            <div class="inst">Assistant Professor<br />ETH Zurich</div>
          </div>
          <div>
            <img src="people/karen.jpg" alt="C. Karen Liu" />
            <div>
              <a href="https://profiles.stanford.edu/c-karen-liu"
                >C. Karen Liu</a
              >
            </div>
            <div class="inst"> Professor<br />Stanford University</div>
          </div>
          <div>
            <img src="people/richard.jpg" alt="Richard Newcombe" />
            <div>
              <a
                href="https://scholar.google.co.uk/citations?user=MhowvPkAAAAJ&hl=en"
                >Richard Newcombe</a
              >
            </div>
            <div class="inst">VP, Research Scientist<br />Meta</div>
          </div>
        </div>
      </section>
    </main>

    <footer>
      <section>
        <div class="footer-container">
          <div class="logo">
            <svg width="300" height="80">
              <defs>
                <clipPath id="clip">
                  <rect x="0" y="0" width="300" height="80" />
                </clipPath>
              </defs>
              <image
                href="css/iccv.svg"
                width="280"
                height="100"
                clip-path="url(#clip)"
              />
            </svg>
          </div>
          <div class="logo">
            <svg width="300" height="80">
              <defs>
                <clipPath id="clip">
                  <rect x="50" y="0" width="200" height="80" />
                </clipPath>
              </defs>
              <image
                href="css/meta.svg"
                width="300"
                height="80"
                clip-path="url(#clip)"
              />
            </svg>
          </div>
          <div class="right">
            <p>Contact</p>
            <p>
              <i class="fa-solid fa-envelope" style="color: #f4976c"></i>
              &nbsp;lingni.ma@meta.com
            </p>
          </div>
        </div>
      </section>
    </footer>
  </body>
</html>
